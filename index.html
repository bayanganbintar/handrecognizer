<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Custom Hand Gesture Detection</title>
    <style>
      body {
        font-family: sans-serif;
        text-align: center;
        padding: 20px;
      }
      video {
        display: none;
      }
      #gestureLabel {
        font-size: 2rem;
        font-weight: bold;
        margin-top: 1rem;
      }
    </style>
  </head>
  <body>
    <h1>Hand Gesture Detection</h1>
    <button id="webcamButton">Enable Webcam</button>
    <video id="webcam" autoplay playsinline></video>
    <div id="gestureLabel"></div>

    <script type="module">
      import {
        HandLandmarker,
        FilesetResolver,
      } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

      const video = document.getElementById("webcam");
      const gestureLabel = document.getElementById("gestureLabel");
      const webcamButton = document.getElementById("webcamButton");
      let handLandmarker;
      let runningMode = "VIDEO";

      // Define custom gestures
      function detectPointing(lm) {
        return (
          lm[8].y < lm[6].y && [12, 16, 20].every((i) => lm[i].y > lm[i - 2].y)
        );
      }

      function detectClosedHand(lm) {
        return [8, 12, 16, 20].every((i) => lm[i].y > lm[i - 2].y);
      }

      async function initLandmarker() {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );

        handLandmarker = await HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath:
              "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
            delegate: "GPU",
          },
          runningMode,
          numHands: 1,
        });
      }

      async function startWebcam() {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
        requestAnimationFrame(predictWebcam);
      }

      async function predictWebcam() {
        if (!handLandmarker) return;

        const now = performance.now();
        const results = handLandmarker.detectForVideo(video, now);

        if (results.landmarks && results.landmarks.length > 0) {
          const lm = results.landmarks[0];
          if (detectPointing(lm)) gestureLabel.textContent = "ðŸ‘‰ Pointing";
          else if (detectClosedHand(lm)) gestureLabel.textContent = "âœŠ Closed Hand";
          else gestureLabel.textContent = "";
        }

        requestAnimationFrame(predictWebcam);
      }

      webcamButton.onclick = async () => {
        webcamButton.disabled = true;
        await initLandmarker();
        await startWebcam();
      };
    </script>
  </body>
</html>
